# Projeto de PortfÃ³lio: Pipeline de ETL e Dashboard de Vendas

## ðŸŽ¯ Objetivo
Este projeto implementa um pipeline de ETL completo para processar dados de vendas do dataset pÃºblico da Olist. Os dados brutos sÃ£o extraÃ­dos de arquivos CSV, transformados com Python (Pandas), carregados em um banco de dados PostgreSQL e, por fim, visualizados em um dashboard interativo.

## ðŸš€ Tecnologias Utilizadas
- **Linguagem:** Python
- **Bibliotecas:** Pandas, SQLAlchemy
- **Banco de Dados:** PostgreSQL
- **Ferramenta de BI:** Google Looker Studio
- **Ambiente:** Jupyter Notebook (para exploraÃ§Ã£o) e script .py (para automaÃ§Ã£o)

## ðŸ“ Estrutura do Projeto
- `etl.py`: Script principal que executa todo o pipeline de ETL de forma automatizada.
- `exploracao_etl.ipynb`: Jupyter Notebook contendo a anÃ¡lise exploratÃ³ria, o desenvolvimento da lÃ³gica de ETL e as queries SQL para a criaÃ§Ã£o do dashboard.
- `dados/`: ContÃ©m os datasets brutos em formato CSV.
- `dashboard_data/`: ContÃ©m os dados agregados usados como fonte para o dashboard.
- `schema.sql`: ContÃ©m o cÃ³digo SQL para a criaÃ§Ã£o das tabelas no PostgreSQL.

## ðŸ“Š Dashboard
O dashboard interativo apresenta os principais indicadores de negÃ³cio, como receita mensal, top categorias de produtos e distribuiÃ§Ã£o de clientes por estado.

**[>> Clique aqui para ver o dashboard interativo <<](https://lookerstudio.google.com/reporting/74848e0d-43a6-4637-bb43-b380ca1b9603)**

## âš™ï¸ Como Executar

Para replicar este projeto em seu ambiente local, siga os passos abaixo.

### 1. PrÃ©-requisitos
- Ter o [Python 3](https://www.python.org/downloads/) instalado.
- Ter o [PostgreSQL](https://www.postgresql.org/download/) instalado. O **pgAdmin**, que Ã© a interface grÃ¡fica, geralmente Ã© incluÃ­do na instalaÃ§Ã£o.

### 2. ConfiguraÃ§Ã£o do Banco de Dados
1.  **Crie o Banco de Dados:** Usando o **pgAdmin**, crie um novo banco de dados com o nome `olist_db`.
2.  **Crie as Tabelas:** Abra a "Query Tool" (Ferramenta de Consulta) para o banco `olist_db`. Copie todo o conteÃºdo do arquivo `schema.sql` deste repositÃ³rio, cole na Query Tool e execute o script. Isso criarÃ¡ todas as tabelas e relacionamentos necessÃ¡rios.

### 3. ConfiguraÃ§Ã£o do Projeto
1.  **Clone o RepositÃ³rio:**
    ```bash
    git clone https://github.com/AndreVVoigt/projeto-etl-olist
    ```
2.  **Instale as DependÃªncias:**
    ```bash
    pip install pandas sqlalchemy psycopg2-binary
    ```
3.  **Adicione os Dados Brutos:** Baixe o dataset da Olist do Kaggle [https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce?resource=download](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce?resource=download) e coloque todos os arquivos `.csv` dentro da pasta `dados/`.
4.  **Configure a ConexÃ£o:** No arquivo `etl.py`, atualize a variÃ¡vel `db_password` com a sua senha do PostgreSQL.

### 4. ExecuÃ§Ã£o do Pipeline
1.  Execute o script de ETL atravÃ©s do terminal:
    ```bash
    python etl.py
    ```
2.  O script irÃ¡ limpar as tabelas (se jÃ¡ tiverem dados) e carregar os dados processados do CSV para o PostgreSQL.
3.  ApÃ³s a execuÃ§Ã£o, vocÃª pode usar o **pgAdmin** para se conectar ao banco `olist_db` e verificar se os dados foram carregados corretamente nas tabelas.
